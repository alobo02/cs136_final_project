{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a700a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1d16e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAPestimator():\n",
    "    \"\"\"\n",
    "    Maximum A-Posteriori Estimator for musk probabilities\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    w_D   : D-dimensional vector of reals\n",
    "            Defines weight vector\n",
    "    alpha : float, must be greater than 0\n",
    "            Defines precision parameter of the multivariate Gaussian prior on the weight vector\n",
    "    iteration_count: integer\n",
    "            Defines number of iterations it took for model to converge on last call of fit()\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> word_list = ['dinosaur', 'trex', 'dinosaur', 'stegosaurus']\n",
    "    >>> mapEst = MAPEstimator(Vocabulary(word_list), alpha=2.0)\n",
    "    >>> mapEst.fit(word_list)\n",
    "    >>> np.allclose(mapEst.predict_proba('dinosaur'), 3.0/7.0)\n",
    "    True\n",
    "\n",
    "    >>> mapEst.predict_proba('never_seen-before')\n",
    "    Traceback (most recent call last):\n",
    "    ...\n",
    "    KeyError: 'Word never_seen-before not in the vocabulary'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w_D, alpha=1.0):\n",
    "        self.w_D = w_D\n",
    "        self.alpha = float(alpha)\n",
    "        self.iteration_count = 0\n",
    "        \n",
    "\n",
    "    def fit(self, train_X, train_y, step_size = 1.0, max_iter = 100):\n",
    "        ''' Fit this estimator to provided training data with first order stochastic gradient descent\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        train_X : NxD array\n",
    "            Each entry is a D-dimensional vector representing a training example\n",
    "        train_y : Nx1 array\n",
    "            Each entry is corresponding output class for training data\n",
    "        step_size: float must be greater than 0\n",
    "            The step size of the gradient descent algorithm\n",
    "        max_iter: int greater than 0\n",
    "            Maximum number of iterations that the gradient descent algorithm will take\n",
    "                    \n",
    "        Returns\n",
    "        -------\n",
    "        None. Internal attributes updated.\n",
    "\n",
    "        Post Condition\n",
    "        --------------\n",
    "        Attributes will updated based on provided word list\n",
    "        * The 1D array count_V is set to the count of each vocabulary word\n",
    "        * The integer total_count is set to the total length of the word list\n",
    "        '''\n",
    "        self.iteration_count = 0\n",
    "        \n",
    "        example_num = 0\n",
    "        num_examples = len(train_X)\n",
    "        \n",
    "        while(self.iteration_count <= max_iter):\n",
    "            h_x = np.dot(self.w_D, train_X[example_num])\n",
    "            sig = 1 / (1 + exp(-h_x))\n",
    "            self.w_D = self.w_D - step_size * (sig - test_y[example_num]) * train_X[example_num] + self.alpha * self.w_D\n",
    "            example_num += 1\n",
    "            if example_num >= num_examples:\n",
    "                example_num = 0\n",
    "            self.iteration_count += 1\n",
    "\n",
    "    def predict_proba(self, test_X):\n",
    "        ''' Predict probability of a given set of feature vectors under this model\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        test_X : NxD vector\n",
    "            Examples for which probability will be predicted\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        proba : float between 0 and 1\n",
    "\n",
    "        Throws\n",
    "        ------\n",
    "        ValueError if hyperparameters do not allow MAP estimation\n",
    "        KeyError if the provided word is not in the vocabulary\n",
    "        '''\n",
    "\n",
    "        # TODO calculate MAP estimate of the provided word\n",
    "        \n",
    "        lin_preds = np.matmul(test_X, self.w_D)\n",
    "        \n",
    "        sig_preds = 1 / (1 + exp(-lin_preds))\n",
    "        \n",
    "        return sig_preds\n",
    "\n",
    "    def score(self, test_X, test_y):\n",
    "        ''' Compute the average log probability of words in provided list\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        test_X : NxD array\n",
    "            Each entry is a D-dimensional vector representing a test example\n",
    "        test_y : Nx1 array\n",
    "            Each entry is corresponding output class for test data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        avg_log_proba : float between (-np.inf, 0.0)\n",
    "        '''\n",
    "        correct_count = 0\n",
    "        num_examples = len(test_X)\n",
    "    \n",
    "        for i in range(num_examples):\n",
    "            pred_proba = self.predict_proba(test_X[i])\n",
    "            if pred_proba < 0.5:\n",
    "                pred_class = 0\n",
    "            else:\n",
    "                pred_class = 1\n",
    "                \n",
    "            if (pred_class == test_y[i]):\n",
    "                correct_count += 1\n",
    "            \n",
    "            \n",
    "        return correct_count / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4011446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
